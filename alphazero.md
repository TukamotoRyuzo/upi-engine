# 学習過程

- 自己対戦フェーズ
  - 1回の学習サイクルで25000局の対戦データ(s, π, z)を生成
    - s: 状態
    - π: 方策
    - z: 評価値
  - 学習開始局面はランダマイズする
- 学習フェーズ
  - 作成したデータでNNを学習
  - 用いるニューラルネットはAlpha Zeroと同じ構造
  - 各手のMCTSは800回（Alpha Zeroの論文と同等）
  - 自己対局で生成するゲーム数は300ゲーム（15CPU並列で20ゲームずつ、対称性考慮で2400ゲーム、Alpha Zeroでは25000）
  - 保存する手の数は150万（1対局約60手として25000ゲーム、Alpha Zeroでは50万ゲーム）
  - 1回のトレーニングは64バッチ×32回を1セットとしてそれを500回繰り返す（Alpha Zeroでは1000回繰り返し）
  - 学習率は0.01（Alpha Zeroでは0.2から始めて一定回数に達すると1/10にしている）
- 性能評価フェーズ
  - 新旧NNで400回対戦させ、新型が55%以上勝利したらそのモデルを採用



